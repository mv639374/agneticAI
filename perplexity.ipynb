{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c856f69f",
   "metadata": {},
   "source": [
    "Excellent! Now let's proceed to **Step 1.2: Database Layer** - setting up PostgreSQL, Redis, and Vector Store with LangGraph checkpointing.[1][2][3][4]\n",
    "\n",
    "***\n",
    "\n",
    "## **STEP 1.2: Database Layer Setup**\n",
    "\n",
    "### **File 1: `backend/app/db/__init__.py`**\n",
    "\n",
    "```python\n",
    "\"\"\"Database layer for PostgreSQL, Redis, and Vector Store.\"\"\"\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### **File 2: `backend/app/db/postgres.py`**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "PostgreSQL Database Connection using SQLAlchemy 2.0 Async\n",
    "\n",
    "This module handles:\n",
    "1. Async database session management\n",
    "2. Connection pooling for performance\n",
    "3. LangGraph checkpointer integration for agent state persistence\n",
    "4. Hot reload mechanism for caching frequently accessed data\n",
    "\n",
    "Key Concepts:\n",
    "- AsyncEngine: Async-compatible database engine\n",
    "- AsyncSession: Async database session for queries\n",
    "- sessionmaker: Factory for creating sessions\n",
    "- Connection pooling: Reuses connections for better performance\n",
    "\"\"\"\n",
    "\n",
    "from contextlib import asynccontextmanager\n",
    "from typing import AsyncGenerator\n",
    "\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.ext.asyncio import (\n",
    "    AsyncEngine,\n",
    "    AsyncSession,\n",
    "    async_sessionmaker,\n",
    "    create_async_engine,\n",
    ")\n",
    "from sqlalchemy.orm import DeclarativeBase\n",
    "from sqlalchemy.pool import NullPool, QueuePool\n",
    "\n",
    "from app.config import settings\n",
    "from app.utils.logger import get_logger\n",
    "\n",
    "log = get_logger(__name__)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BASE MODEL FOR DECLARATIVE ORM\n",
    "# =============================================================================\n",
    "class Base(DeclarativeBase):\n",
    "    \"\"\"\n",
    "    Base class for all SQLAlchemy models.\n",
    "    \n",
    "    All database models inherit from this class to get SQLAlchemy's ORM features.\n",
    "    \n",
    "    Example:\n",
    "        class User(Base):\n",
    "            __tablename__ = \"users\"\n",
    "            id = Column(Integer, primary_key=True)\n",
    "            name = Column(String)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATABASE ENGINE CONFIGURATION\n",
    "# =============================================================================\n",
    "def create_database_engine() -> AsyncEngine:\n",
    "    \"\"\"\n",
    "    Create async PostgreSQL engine with connection pooling.\n",
    "    \n",
    "    Connection Pool Settings:\n",
    "    - pool_size: Number of connections to keep open (default: 5)\n",
    "    - max_overflow: Additional connections beyond pool_size (default: 10)\n",
    "    - pool_pre_ping: Check connection health before using (prevents stale connections)\n",
    "    - pool_recycle: Recycle connections after N seconds (prevents timeout)\n",
    "    - echo: Log all SQL statements (useful for debugging)\n",
    "    \n",
    "    Returns:\n",
    "        AsyncEngine: Configured async database engine\n",
    "    \"\"\"\n",
    "    \n",
    "    # Development: Use NullPool for easier debugging (no connection reuse)\n",
    "    # Production: Use QueuePool for better performance\n",
    "    poolclass = NullPool if settings.DEBUG else QueuePool\n",
    "    \n",
    "    engine = create_async_engine(\n",
    "        settings.DATABASE_URL,\n",
    "        echo=settings.DEBUG,  # Log SQL in debug mode\n",
    "        poolclass=poolclass,\n",
    "        pool_size=5,  # Keep 5 connections open\n",
    "        max_overflow=10,  # Allow up to 15 total connections (5 + 10)\n",
    "        pool_pre_ping=True,  # Verify connections are alive\n",
    "        pool_recycle=3600,  # Recycle connections after 1 hour\n",
    "    )\n",
    "    \n",
    "    log.info(\n",
    "        \"Database engine created\",\n",
    "        pool_class=poolclass.__name__,\n",
    "        pool_size=5 if poolclass == QueuePool else 0,\n",
    "    )\n",
    "    \n",
    "    return engine\n",
    "\n",
    "\n",
    "# Create global engine instance\n",
    "engine = create_database_engine()\n",
    "\n",
    "# Create session factory\n",
    "# expire_on_commit=False prevents SQLAlchemy from expiring objects after commit\n",
    "# This is important for async code to avoid lazy-loading issues\n",
    "AsyncSessionLocal = async_sessionmaker(\n",
    "    engine,\n",
    "    class_=AsyncSession,\n",
    "    expire_on_commit=False,\n",
    "    autocommit=False,\n",
    "    autoflush=False,\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATABASE SESSION DEPENDENCY\n",
    "# =============================================================================\n",
    "@asynccontextmanager\n",
    "async def get_db_session() -> AsyncGenerator[AsyncSession, None]:\n",
    "    \"\"\"\n",
    "    Async context manager for database sessions.\n",
    "    \n",
    "    This is a dependency injection pattern for FastAPI endpoints.\n",
    "    It ensures:\n",
    "    1. Session is created for each request\n",
    "    2. Session is automatically committed on success\n",
    "    3. Session is rolled back on exception\n",
    "    4. Session is always closed\n",
    "    \n",
    "    Usage in FastAPI:\n",
    "        @app.get(\"/users\")\n",
    "        async def get_users(db: AsyncSession = Depends(get_db_session)):\n",
    "            result = await db.execute(select(User))\n",
    "            return result.scalars().all()\n",
    "    \n",
    "    Yields:\n",
    "        AsyncSession: Database session for this request\n",
    "    \"\"\"\n",
    "    async with AsyncSessionLocal() as session:\n",
    "        try:\n",
    "            yield session\n",
    "            await session.commit()\n",
    "        except Exception as e:\n",
    "            await session.rollback()\n",
    "            log.error(\"Database transaction failed\", exc_info=e)\n",
    "            raise\n",
    "        finally:\n",
    "            await session.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATABASE INITIALIZATION\n",
    "# =============================================================================\n",
    "async def init_database() -> None:\n",
    "    \"\"\"\n",
    "    Initialize database by creating all tables.\n",
    "    \n",
    "    This should be called once at application startup.\n",
    "    It creates tables for:\n",
    "    - Conversations\n",
    "    - Agent states\n",
    "    - LangGraph checkpoints\n",
    "    - User data\n",
    "    \"\"\"\n",
    "    async with engine.begin() as conn:\n",
    "        # Create all tables defined in Base subclasses\n",
    "        await conn.run_sync(Base.metadata.create_all)\n",
    "    \n",
    "    log.info(\"Database tables created/verified\")\n",
    "\n",
    "\n",
    "async def check_database_connection() -> bool:\n",
    "    \"\"\"\n",
    "    Check if database connection is working.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if connection is healthy, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        async with engine.begin() as conn:\n",
    "            result = await conn.execute(text(\"SELECT 1\"))\n",
    "            result.scalar()\n",
    "        log.info(\"Database connection verified\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log.error(\"Database connection failed\", exc_info=e)\n",
    "        return False\n",
    "\n",
    "\n",
    "async def close_database() -> None:\n",
    "    \"\"\"\n",
    "    Close all database connections.\n",
    "    \n",
    "    This should be called at application shutdown.\n",
    "    \"\"\"\n",
    "    await engine.dispose()\n",
    "    log.info(\"Database connections closed\")\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### **File 3: `backend/app/db/models.py`**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "SQLAlchemy Models for Database Tables\n",
    "\n",
    "This module defines:\n",
    "1. Conversation model - stores user conversations\n",
    "2. AgentExecution model - stores agent execution history\n",
    "3. Checkpoint model - LangGraph state checkpoints (handled by langgraph-checkpoint-postgres)\n",
    "\n",
    "Database Schema Design:\n",
    "- Conversations: Main conversation threads\n",
    "- AgentExecutions: Individual agent task executions within conversations\n",
    "- Relationship: One conversation has many agent executions\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from typing import Optional\n",
    "from uuid import uuid4\n",
    "\n",
    "from sqlalchemy import JSON, DateTime, ForeignKey, Integer, String, Text, func\n",
    "from sqlalchemy.orm import Mapped, mapped_column, relationship\n",
    "\n",
    "from app.db.postgres import Base\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONVERSATION MODEL\n",
    "# =============================================================================\n",
    "class Conversation(Base):\n",
    "    \"\"\"\n",
    "    Stores conversation threads between users and the multi-agent system.\n",
    "    \n",
    "    Each conversation is a thread where:\n",
    "    - User submits queries\n",
    "    - Supervisor delegates to agents\n",
    "    - Agents execute tasks and return results\n",
    "    \n",
    "    Attributes:\n",
    "        id: Unique conversation identifier (UUID)\n",
    "        title: Human-readable conversation title\n",
    "        user_id: User who initiated the conversation (optional for Phase 1)\n",
    "        created_at: Timestamp when conversation started\n",
    "        updated_at: Timestamp of last activity\n",
    "        metadata: Additional context (user preferences, session info, etc.)\n",
    "        agent_executions: Related agent execution records\n",
    "    \"\"\"\n",
    "    \n",
    "    __tablename__ = \"conversations\"\n",
    "    \n",
    "    # Primary key using UUID for distributed systems compatibility\n",
    "    id: Mapped[str] = mapped_column(\n",
    "        String(36),\n",
    "        primary_key=True,\n",
    "        default=lambda: str(uuid4()),\n",
    "        comment=\"Unique conversation identifier\"\n",
    "    )\n",
    "    \n",
    "    # Conversation metadata\n",
    "    title: Mapped[str] = mapped_column(\n",
    "        String(255),\n",
    "        nullable=False,\n",
    "        default=\"New Conversation\",\n",
    "        comment=\"Human-readable conversation title\"\n",
    "    )\n",
    "    \n",
    "    user_id: Mapped[Optional[str]] = mapped_column(\n",
    "        String(36),\n",
    "        nullable=True,\n",
    "        comment=\"User who owns this conversation (optional in Phase 1)\"\n",
    "    )\n",
    "    \n",
    "    # Timestamps with automatic updates\n",
    "    created_at: Mapped[datetime] = mapped_column(\n",
    "        DateTime(timezone=True),\n",
    "        server_default=func.now(),\n",
    "        nullable=False,\n",
    "        comment=\"Conversation creation timestamp\"\n",
    "    )\n",
    "    \n",
    "    updated_at: Mapped[datetime] = mapped_column(\n",
    "        DateTime(timezone=True),\n",
    "        server_default=func.now(),\n",
    "        onupdate=func.now(),\n",
    "        nullable=False,\n",
    "        comment=\"Last update timestamp\"\n",
    "    )\n",
    "    \n",
    "    # JSON metadata for flexible storage\n",
    "    metadata: Mapped[Optional[dict]] = mapped_column(\n",
    "        JSON,\n",
    "        nullable=True,\n",
    "        comment=\"Additional conversation context (preferences, tags, etc.)\"\n",
    "    )\n",
    "    \n",
    "    # Relationship: One conversation has many agent executions\n",
    "    agent_executions: Mapped[list[\"AgentExecution\"]] = relationship(\n",
    "        \"AgentExecution\",\n",
    "        back_populates=\"conversation\",\n",
    "        cascade=\"all, delete-orphan\",  # Delete executions when conversation is deleted\n",
    "    )\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"<Conversation(id={self.id}, title={self.title})>\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# AGENT EXECUTION MODEL\n",
    "# =============================================================================\n",
    "class AgentExecution(Base):\n",
    "    \"\"\"\n",
    "    Stores individual agent task executions within a conversation.\n",
    "    \n",
    "    Each execution represents:\n",
    "    - Which agent was invoked\n",
    "    - Input provided to the agent\n",
    "    - Output/result from the agent\n",
    "    - Execution metadata (duration, tokens used, etc.)\n",
    "    \n",
    "    This enables:\n",
    "    - Conversation history replay\n",
    "    - Agent performance monitoring\n",
    "    - Debugging agent behavior\n",
    "    - Cost tracking (LLM token usage)\n",
    "    \"\"\"\n",
    "    \n",
    "    __tablename__ = \"agent_executions\"\n",
    "    \n",
    "    # Primary key\n",
    "    id: Mapped[int] = mapped_column(\n",
    "        Integer,\n",
    "        primary_key=True,\n",
    "        autoincrement=True,\n",
    "        comment=\"Unique execution identifier\"\n",
    "    )\n",
    "    \n",
    "    # Foreign key to conversation\n",
    "    conversation_id: Mapped[str] = mapped_column(\n",
    "        String(36),\n",
    "        ForeignKey(\"conversations.id\", ondelete=\"CASCADE\"),\n",
    "        nullable=False,\n",
    "        index=True,  # Index for faster joins\n",
    "        comment=\"Parent conversation ID\"\n",
    "    )\n",
    "    \n",
    "    # Agent information\n",
    "    agent_name: Mapped[str] = mapped_column(\n",
    "        String(100),\n",
    "        nullable=False,\n",
    "        index=True,  # Index for querying by agent\n",
    "        comment=\"Name of the agent that executed this task\"\n",
    "    )\n",
    "    \n",
    "    agent_type: Mapped[str] = mapped_column(\n",
    "        String(50),\n",
    "        nullable=False,\n",
    "        comment=\"Type of agent (supervisor, worker, etc.)\"\n",
    "    )\n",
    "    \n",
    "    # Execution data\n",
    "    input_data: Mapped[dict] = mapped_column(\n",
    "        JSON,\n",
    "        nullable=False,\n",
    "        comment=\"Input provided to the agent\"\n",
    "    )\n",
    "    \n",
    "    output_data: Mapped[Optional[dict]] = mapped_column(\n",
    "        JSON,\n",
    "        nullable=True,\n",
    "        comment=\"Output/result from the agent\"\n",
    "    )\n",
    "    \n",
    "    # Status tracking\n",
    "    status: Mapped[str] = mapped_column(\n",
    "        String(20),\n",
    "        nullable=False,\n",
    "        default=\"pending\",\n",
    "        comment=\"Execution status: pending, running, completed, failed\"\n",
    "    )\n",
    "    \n",
    "    error_message: Mapped[Optional[str]] = mapped_column(\n",
    "        Text,\n",
    "        nullable=True,\n",
    "        comment=\"Error message if execution failed\"\n",
    "    )\n",
    "    \n",
    "    # Timestamps\n",
    "    started_at: Mapped[datetime] = mapped_column(\n",
    "        DateTime(timezone=True),\n",
    "        server_default=func.now(),\n",
    "        nullable=False,\n",
    "        comment=\"Execution start time\"\n",
    "    )\n",
    "    \n",
    "    completed_at: Mapped[Optional[datetime]] = mapped_column(\n",
    "        DateTime(timezone=True),\n",
    "        nullable=True,\n",
    "        comment=\"Execution completion time\"\n",
    "    )\n",
    "    \n",
    "    # Performance metrics\n",
    "    duration_ms: Mapped[Optional[int]] = mapped_column(\n",
    "        Integer,\n",
    "        nullable=True,\n",
    "        comment=\"Execution duration in milliseconds\"\n",
    "    )\n",
    "    \n",
    "    tokens_used: Mapped[Optional[int]] = mapped_column(\n",
    "        Integer,\n",
    "        nullable=True,\n",
    "        comment=\"Number of LLM tokens consumed\"\n",
    "    )\n",
    "    \n",
    "    # Metadata\n",
    "    metadata: Mapped[Optional[dict]] = mapped_column(\n",
    "        JSON,\n",
    "        nullable=True,\n",
    "        comment=\"Additional execution metadata (model used, temperature, etc.)\"\n",
    "    )\n",
    "    \n",
    "    # Relationship: Many executions belong to one conversation\n",
    "    conversation: Mapped[\"Conversation\"] = relationship(\n",
    "        \"Conversation\",\n",
    "        back_populates=\"agent_executions\"\n",
    "    )\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"<AgentExecution(id={self.id}, agent={self.agent_name}, status={self.status})>\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# NOTES ON LANGGRAPH CHECKPOINTING\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "LangGraph Checkpointing Tables:\n",
    "\n",
    "The langgraph-checkpoint-postgres package automatically creates these tables:\n",
    "1. checkpoints - Stores graph execution state\n",
    "2. checkpoint_blobs - Stores large binary data (embeddings, etc.)\n",
    "3. checkpoint_writes - Stores pending writes\n",
    "\n",
    "These tables enable:\n",
    "- Agent state persistence across requests\n",
    "- Conversation memory (agents remember previous interactions)\n",
    "- Error recovery (resume from last checkpoint)\n",
    "- Time-travel debugging (inspect state at any point)\n",
    "\n",
    "We don't need to define these models manually - they're handled by:\n",
    "    from langgraph.checkpoint.postgres import PostgresSaver\n",
    "    \n",
    "    checkpointer = PostgresSaver.from_conn_string(DATABASE_URL)\n",
    "    await checkpointer.setup()  # Creates tables automatically\n",
    "\n",
    "See implementation in app/graphs/supervisor_graph.py\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### **File 4: `backend/app/db/redis_cache.py`**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Redis Cache Manager with Hot Reload Mechanism\n",
    "\n",
    "This module provides:\n",
    "1. Redis connection management\n",
    "2. Cache operations (get, set, delete)\n",
    "3. Hot reload mechanism (periodically sync from PostgreSQL to local cache)\n",
    "4. Fallback to database when Redis is unavailable\n",
    "\n",
    "Cache Strategy:\n",
    "- Frequently accessed data is cached in Redis\n",
    "- Local in-memory fallback for development\n",
    "- Automatic cache invalidation after TTL\n",
    "- Hot reload refreshes cache from PostgreSQL every N minutes\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "from typing import Any, Optional\n",
    "\n",
    "import redis.asyncio as redis\n",
    "from redis.asyncio import Redis\n",
    "\n",
    "from app.config import settings\n",
    "from app.utils.logger import get_logger\n",
    "\n",
    "log = get_logger(__name__)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# REDIS CONNECTION MANAGEMENT\n",
    "# =============================================================================\n",
    "class RedisCache:\n",
    "    \"\"\"\n",
    "    Redis cache manager with async operations and hot reload.\n",
    "    \n",
    "    Features:\n",
    "    - Async Redis operations\n",
    "    - Connection pooling\n",
    "    - Automatic reconnection\n",
    "    - Fallback to in-memory cache if Redis unavailable\n",
    "    - Hot reload mechanism from PostgreSQL\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.redis_client: Optional[Redis] = None\n",
    "        self._local_cache: dict[str, Any] = {}  # Fallback in-memory cache\n",
    "        self._hot_reload_task: Optional[asyncio.Task] = None\n",
    "    \n",
    "    async def connect(self) -> None:\n",
    "        \"\"\"\n",
    "        Connect to Redis server.\n",
    "        \n",
    "        If connection fails, logs error and falls back to in-memory cache.\n",
    "        \"\"\"\n",
    "        if not settings.ENABLE_CACHE:\n",
    "            log.info(\"Redis caching disabled\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.redis_client = redis.from_url(\n",
    "                settings.REDIS_URL,\n",
    "                encoding=\"utf-8\",\n",
    "                decode_responses=True,\n",
    "                max_connections=10,  # Connection pool size\n",
    "            )\n",
    "            \n",
    "            # Test connection\n",
    "            await self.redis_client.ping()\n",
    "            log.info(\"Redis connection established\", url=settings.REDIS_HOST)\n",
    "            \n",
    "            # Start hot reload task\n",
    "            if settings.CACHE_HOT_RELOAD_INTERVAL > 0:\n",
    "                self._hot_reload_task = asyncio.create_task(self._hot_reload_loop())\n",
    "                log.info(\"Hot reload mechanism started\", interval=settings.CACHE_HOT_RELOAD_INTERVAL)\n",
    "        \n",
    "        except Exception as e:\n",
    "            log.warning(\n",
    "                \"Redis connection failed, using in-memory cache\",\n",
    "                exc_info=e\n",
    "            )\n",
    "            self.redis_client = None\n",
    "    \n",
    "    async def disconnect(self) -> None:\n",
    "        \"\"\"Close Redis connection and stop hot reload.\"\"\"\n",
    "        if self._hot_reload_task:\n",
    "            self._hot_reload_task.cancel()\n",
    "            try:\n",
    "                await self._hot_reload_task\n",
    "            except asyncio.CancelledError:\n",
    "                pass\n",
    "        \n",
    "        if self.redis_client:\n",
    "            await self.redis_client.close()\n",
    "            log.info(\"Redis connection closed\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CACHE OPERATIONS\n",
    "    # =========================================================================\n",
    "    \n",
    "    async def get(self, key: str) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Get value from cache.\n",
    "        \n",
    "        Args:\n",
    "            key: Cache key\n",
    "        \n",
    "        Returns:\n",
    "            Cached value or None if not found\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.redis_client:\n",
    "                value = await self.redis_client.get(key)\n",
    "                if value:\n",
    "                    return json.loads(value)\n",
    "            else:\n",
    "                # Fallback to in-memory cache\n",
    "                return self._local_cache.get(key)\n",
    "        except Exception as e:\n",
    "            log.warning(\"Cache get failed\", key=key, exc_info=e)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    async def set(\n",
    "        self,\n",
    "        key: str,\n",
    "        value: Any,\n",
    "        ttl: Optional[int] = None\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Set value in cache.\n",
    "        \n",
    "        Args:\n",
    "            key: Cache key\n",
    "            value: Value to cache (must be JSON serializable)\n",
    "            ttl: Time-to-live in seconds (default: from settings)\n",
    "        \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        ttl = ttl or settings.CACHE_TTL\n",
    "        \n",
    "        try:\n",
    "            serialized = json.dumps(value)\n",
    "            \n",
    "            if self.redis_client:\n",
    "                await self.redis_client.setex(key, ttl, serialized)\n",
    "            else:\n",
    "                # Fallback to in-memory cache (no TTL in fallback)\n",
    "                self._local_cache[key] = value\n",
    "            \n",
    "            return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            log.warning(\"Cache set failed\", key=key, exc_info=e)\n",
    "            return False\n",
    "    \n",
    "    async def delete(self, key: str) -> bool:\n",
    "        \"\"\"\n",
    "        Delete value from cache.\n",
    "        \n",
    "        Args:\n",
    "            key: Cache key\n",
    "        \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.redis_client:\n",
    "                await self.redis_client.delete(key)\n",
    "            else:\n",
    "                self._local_cache.pop(key, None)\n",
    "            \n",
    "            return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            log.warning(\"Cache delete failed\", key=key, exc_info=e)\n",
    "            return False\n",
    "    \n",
    "    async def clear(self) -> bool:\n",
    "        \"\"\"\n",
    "        Clear all cache entries.\n",
    "        \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.redis_client:\n",
    "                await self.redis_client.flushdb()\n",
    "            else:\n",
    "                self._local_cache.clear()\n",
    "            \n",
    "            log.info(\"Cache cleared\")\n",
    "            return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            log.error(\"Cache clear failed\", exc_info=e)\n",
    "            return False\n",
    "    \n",
    "    # =========================================================================\n",
    "    # HOT RELOAD MECHANISM\n",
    "    # =========================================================================\n",
    "    \n",
    "    async def _hot_reload_loop(self) -> None:\n",
    "        \"\"\"\n",
    "        Background task to periodically reload cache from PostgreSQL.\n",
    "        \n",
    "        This ensures cache stays fresh with database changes made by other services.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                await asyncio.sleep(settings.CACHE_HOT_RELOAD_INTERVAL)\n",
    "                await self._reload_cache()\n",
    "            except asyncio.CancelledError:\n",
    "                break\n",
    "            except Exception as e:\n",
    "                log.error(\"Hot reload failed\", exc_info=e)\n",
    "    \n",
    "    async def _reload_cache(self) -> None:\n",
    "        \"\"\"\n",
    "        Reload frequently accessed data from PostgreSQL to cache.\n",
    "        \n",
    "        TODO: In Step 1.4, this will reload:\n",
    "        - Recent conversation summaries\n",
    "        - Agent performance metrics\n",
    "        - User preferences\n",
    "        \"\"\"\n",
    "        log.debug(\"Hot reload triggered\")\n",
    "        \n",
    "        # TODO: Implement actual reload logic in Step 1.4\n",
    "        # Example:\n",
    "        # async with get_db_session() as db:\n",
    "        #     conversations = await db.execute(select(Conversation).limit(100))\n",
    "        #     for conv in conversations.scalars():\n",
    "        #         await self.set(f\"conv:{conv.id}\", conv.dict())\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# GLOBAL CACHE INSTANCE\n",
    "# =============================================================================\n",
    "cache = RedisCache()\n",
    "\n",
    "\n",
    "async def init_cache() -> None:\n",
    "    \"\"\"Initialize Redis cache. Call at application startup.\"\"\"\n",
    "    await cache.connect()\n",
    "\n",
    "\n",
    "async def close_cache() -> None:\n",
    "    \"\"\"Close Redis cache. Call at application shutdown.\"\"\"\n",
    "    await cache.disconnect()\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### **File 5: `backend/app/db/vector_store.py`**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Vector Store Integration for Semantic Search\n",
    "\n",
    "This module provides vector database integration for:\n",
    "1. Storing conversation embeddings for semantic search\n",
    "2. Agent memory - finding relevant past conversations\n",
    "3. Document embeddings for RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "Supported Vector DBs:\n",
    "- Pinecone (cloud, free tier available)\n",
    "- ChromaDB (self-hosted or cloud)\n",
    "\n",
    "Key Concepts:\n",
    "- Embeddings: Vector representations of text (created by LLMs)\n",
    "- Semantic search: Find similar text based on meaning, not keywords\n",
    "- Namespaces: Logical separation of vectors (e.g., by user)\n",
    "\"\"\"\n",
    "\n",
    "from typing import Any, Optional\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_groq import GroqEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    CHROMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CHROMA_AVAILABLE = False\n",
    "\n",
    "from app.config import settings\n",
    "from app.utils.logger import get_logger\n",
    "\n",
    "log = get_logger(__name__)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EMBEDDING MODEL INITIALIZATION\n",
    "# =============================================================================\n",
    "def get_embedding_model():\n",
    "    \"\"\"\n",
    "    Get embedding model based on configured LLM provider.\n",
    "    \n",
    "    Embeddings convert text to vectors (numerical representations).\n",
    "    Same text always produces same vector, enabling semantic similarity search.\n",
    "    \n",
    "    Providers:\n",
    "    - Google (text-embedding-004): 768 dimensions, good quality\n",
    "    - Groq: Fast inference, compatible with sentence-transformers\n",
    "    \n",
    "    Returns:\n",
    "        Embedding model instance\n",
    "    \"\"\"\n",
    "    if settings.DEFAULT_LLM_PROVIDER == \"google\":\n",
    "        return GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            google_api_key=settings.GOOGLE_API_KEY,\n",
    "        )\n",
    "    else:\n",
    "        # For Groq, we'll use sentence-transformers locally\n",
    "        # This is faster and doesn't consume API quota\n",
    "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "        \n",
    "        return HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={\"device\": \"cpu\"},  # Use GPU if available\n",
    "            encode_kwargs={\"normalize_embeddings\": True},\n",
    "        )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# VECTOR STORE MANAGER\n",
    "# =============================================================================\n",
    "class VectorStoreManager:\n",
    "    \"\"\"\n",
    "    Manages vector database operations for semantic search.\n",
    "    \n",
    "    Provides unified interface for Pinecone and ChromaDB.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vector_store: Optional[Any] = None\n",
    "        self.embeddings = None\n",
    "    \n",
    "    async def initialize(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize vector store based on configuration.\n",
    "        \n",
    "        Creates index/collection if it doesn't exist.\n",
    "        \"\"\"\n",
    "        self.embeddings = get_embedding_model()\n",
    "        \n",
    "        if settings.VECTOR_DB_TYPE == \"pinecone\":\n",
    "            await self._init_pinecone()\n",
    "        elif settings.VECTOR_DB_TYPE == \"chromadb\":\n",
    "            await self._init_chromadb()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported vector DB: {settings.VECTOR_DB_TYPE}\")\n",
    "        \n",
    "        log.info(\"Vector store initialized\", db_type=settings.VECTOR_DB_TYPE)\n",
    "    \n",
    "    async def _init_pinecone(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Pinecone vector store.\n",
    "        \n",
    "        Pinecone is a managed vector database with:\n",
    "        - Free tier: 1 index, 100K vectors\n",
    "        - Serverless: Auto-scaling\n",
    "        - Low latency: Global deployment\n",
    "        \"\"\"\n",
    "        from pinecone import Pinecone, ServerlessSpec\n",
    "        \n",
    "        # Initialize Pinecone client\n",
    "        pc = Pinecone(api_key=settings.PINECONE_API_KEY)\n",
    "        \n",
    "        # Create index if it doesn't exist\n",
    "        index_name = settings.PINECONE_INDEX_NAME\n",
    "        \n",
    "        if index_name not in pc.list_indexes().names():\n",
    "            log.info(\"Creating Pinecone index\", index=index_name)\n",
    "            \n",
    "            # Get embedding dimension (768 for text-embedding-004, 384 for MiniLM)\n",
    "            dimension = 768 if settings.DEFAULT_LLM_PROVIDER == \"google\" else 384\n",
    "            \n",
    "            pc.create_index(\n",
    "                name=index_name,\n",
    "                dimension=dimension,\n",
    "                metric=\"cosine\",  # Similarity metric\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud=\"aws\",\n",
    "                    region=settings.PINECONE_ENVIRONMENT,\n",
    "                ),\n",
    "            )\n",
    "            log.info(\"Pinecone index created\")\n",
    "        \n",
    "        # Initialize LangChain vector store\n",
    "        self.vector_store = PineconeVectorStore(\n",
    "            index_name=index_name,\n",
    "            embedding=self.embeddings,\n",
    "        )\n",
    "    \n",
    "    async def _init_chromadb(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize ChromaDB vector store.\n",
    "        \n",
    "        ChromaDB is an open-source vector database with:\n",
    "        - Self-hosted or cloud\n",
    "        - Embedded mode for development\n",
    "        - Persistent storage\n",
    "        \"\"\"\n",
    "        if not CHROMA_AVAILABLE:\n",
    "            raise ImportError(\"ChromaDB not installed. Run: uv pip install chromadb\")\n",
    "        \n",
    "        # Use persistent storage in data directory\n",
    "        persist_directory = \"data/chromadb\"\n",
    "        \n",
    "        self.vector_store = Chroma(\n",
    "            collection_name=settings.CHROMA_COLLECTION_NAME,\n",
    "            embedding_function=self.embeddings,\n",
    "            persist_directory=persist_directory,\n",
    "        )\n",
    "        \n",
    "        log.info(\"ChromaDB initialized\", persist_dir=persist_directory)\n",
    "    \n",
    "    async def add_texts(\n",
    "        self,\n",
    "        texts: list[str],\n",
    "        metadatas: Optional[list[dict]] = None,\n",
    "        namespace: Optional[str] = None,\n",
    "    ) -> list[str]:\n",
    "        \"\"\"\n",
    "        Add texts to vector store.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed and store\n",
    "            metadatas: Optional metadata for each text\n",
    "            namespace: Logical separation (e.g., user ID, conversation ID)\n",
    "        \n",
    "        Returns:\n",
    "            List of document IDs\n",
    "        \"\"\"\n",
    "        if not self.vector_store:\n",
    "            raise RuntimeError(\"Vector store not initialized\")\n",
    "        \n",
    "        ids = await self.vector_store.aadd_texts(\n",
    "            texts=texts,\n",
    "            metadatas=metadatas,\n",
    "        )\n",
    "        \n",
    "        log.info(\"Texts added to vector store\", count=len(texts), namespace=namespace)\n",
    "        return ids\n",
    "    \n",
    "    async def similarity_search(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 5,\n",
    "        namespace: Optional[str] = None,\n",
    "    ) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Search for similar texts.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query text\n",
    "            k: Number of results to return\n",
    "            namespace: Search within specific namespace\n",
    "        \n",
    "        Returns:\n",
    "            List of similar documents with metadata\n",
    "        \"\"\"\n",
    "        if not self.vector_store:\n",
    "            raise RuntimeError(\"Vector store not initialized\")\n",
    "        \n",
    "        results = await self.vector_store.asimilarity_search(\n",
    "            query=query,\n",
    "            k=k,\n",
    "        )\n",
    "        \n",
    "        log.debug(\"Similarity search completed\", query=query[:50], results=len(results))\n",
    "        return results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# GLOBAL VECTOR STORE INSTANCE\n",
    "# =============================================================================\n",
    "vector_store_manager = VectorStoreManager()\n",
    "\n",
    "\n",
    "async def init_vector_store() -> None:\n",
    "    \"\"\"Initialize vector store. Call at application startup.\"\"\"\n",
    "    await vector_store_manager.initialize()\n",
    "\n",
    "\n",
    "async def close_vector_store() -> None:\n",
    "    \"\"\"Cleanup vector store. Call at application shutdown.\"\"\"\n",
    "    # No cleanup needed for current implementations\n",
    "    log.info(\"Vector store closed\")\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### **File 6: Update `backend/app/main.py`** (Add database initialization)\n",
    "\n",
    "Replace the lifespan function with:\n",
    "\n",
    "```python\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Lifespan events for FastAPI application.\"\"\"\n",
    "    # Startup\n",
    "    log.info(\"Application starting\", environment=settings.ENVIRONMENT, debug=settings.DEBUG)\n",
    "    \n",
    "    # Initialize databases\n",
    "    from app.db.postgres import init_database, check_database_connection\n",
    "    from app.db.redis_cache import init_cache\n",
    "    from app.db.vector_store import init_vector_store\n",
    "    \n",
    "    # Check PostgreSQL connection\n",
    "    if await check_database_connection():\n",
    "        await init_database()\n",
    "    else:\n",
    "        log.error(\"Failed to connect to PostgreSQL\")\n",
    "    \n",
    "    # Initialize Redis cache\n",
    "    await init_cache()\n",
    "    \n",
    "    # Initialize vector store\n",
    "    await init_vector_store()\n",
    "    \n",
    "    log.info(\"All database connections initialized\")\n",
    "    \n",
    "    yield  # Application runs here\n",
    "    \n",
    "    # Shutdown\n",
    "    log.info(\"Application shutting down\")\n",
    "    \n",
    "    from app.db.postgres import close_database\n",
    "    from app.db.redis_cache import close_cache\n",
    "    from app.db.vector_store import close_vector_store\n",
    "    \n",
    "    await close_database()\n",
    "    await close_cache()\n",
    "    await close_vector_store()\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "## **CHECKPOINT TEST: Verify Database Connections**\n",
    "\n",
    "```bash\n",
    "# From backend/ directory with venv activated\n",
    "\n",
    "# Test PostgreSQL connection (update .env with your cloud PostgreSQL URL first)\n",
    "python -c \"\n",
    "import asyncio\n",
    "from app.db.postgres import check_database_connection, init_database\n",
    "\n",
    "async def test():\n",
    "    connected = await check_database_connection()\n",
    "    if connected:\n",
    "        await init_database()\n",
    "        print('✓ PostgreSQL connected and tables created')\n",
    "    else:\n",
    "        print('✗ PostgreSQL connection failed')\n",
    "\n",
    "asyncio.run(test())\n",
    "\"\n",
    "\n",
    "# Test Redis connection (update .env with your Redis URL)\n",
    "python -c \"\n",
    "import asyncio\n",
    "from app.db.redis_cache import cache\n",
    "\n",
    "async def test():\n",
    "    await cache.connect()\n",
    "    await cache.set('test_key', {'hello': 'world'})\n",
    "    value = await cache.get('test_key')\n",
    "    print(f'✓ Redis connected: {value}')\n",
    "    await cache.disconnect()\n",
    "\n",
    "asyncio.run(test())\n",
    "\"\n",
    "\n",
    "# Test Vector Store (Pinecone or ChromaDB)\n",
    "python -c \"\n",
    "import asyncio\n",
    "from app.db.vector_store import vector_store_manager\n",
    "\n",
    "async def test():\n",
    "    await vector_store_manager.initialize()\n",
    "    print('✓ Vector store initialized')\n",
    "\n",
    "asyncio.run(test())\n",
    "\"\n",
    "\n",
    "# Start server and verify all connections\n",
    "uv run uvicorn app.main:app --reload\n",
    "\n",
    "# In another terminal:\n",
    "curl http://localhost:8000/api/health\n",
    "# Should see: {\"status\":\"healthy\",\"environment\":\"development\",\"version\":\"0.1.0\"}\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "**Reply \"done\" when all database connections are working, then we'll proceed to Step 1.3: MCP Tools Implementation**.[2][3][4][1]\n",
    "\n",
    "[1](https://github.com/seapagan/fastapi_async_sqlalchemy2_example)\n",
    "[2](https://www.reddit.com/r/LangGraph/comments/1njhmrh/langgraph_checkpointer_issue_with_postgresql/)\n",
    "[3](https://langchain-ai.github.io/langgraph/how-tos/memory/add-memory/)\n",
    "[4](https://python.langchain.com/docs/integrations/vectorstores/pinecone/)\n",
    "[5](https://www.youtube.com/watch?v=gg7AX1iRnmg)\n",
    "[6](https://dev.to/akarshan/asynchronous-database-sessions-in-fastapi-with-sqlalchemy-1o7e)\n",
    "[7](https://testdriven.io/blog/fastapi-sqlmodel/)\n",
    "[8](https://github.com/reinhud/async-fastapi-postgres-template)\n",
    "[9](https://api.python.langchain.com/en/latest/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html)\n",
    "[10](https://craftyourstartup.com/cys-docs/setup-postgresql-fastapi-guide/)\n",
    "[11](https://www.npmjs.com/package/@langchain/langgraph-checkpoint-postgres)\n",
    "[12](https://www.ai-for-devs.com/blog/how)\n",
    "[13](https://www.reddit.com/r/Python/comments/yrelvq/best_approach_for_async_sqlalchemy_in_fastapi/)\n",
    "[14](https://airbyte.com/data-engineering-resources/pinecone-vector-database)\n",
    "[15](https://www.reddit.com/r/FastAPI/comments/11vwn1p/fastapi_with_async_sqlalchemy_20_and_alembic/)\n",
    "[16](https://pydigger.com/pypi/langgraph-checkpoint-postgres)\n",
    "[17](https://berkkaraal.com/blog/2024/09/19/setup-fastapi-project-with-async-sqlalchemy-2-alembic-postgresql-and-docker/)\n",
    "[18](https://langchain-ai.github.io/langgraphjs/how-tos/persistence-postgres/)\n",
    "[19](https://docs.pinecone.io/reference/python-sdk)\n",
    "[20](https://www.youtube.com/watch?v=cH0immwfykI)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
